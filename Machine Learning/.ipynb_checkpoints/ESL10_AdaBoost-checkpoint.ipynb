{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "Ensemble methods:\n",
    "<img src=\"images/ensemble_outline.jpg\" width=\"500\"/>\n",
    "\n",
    "**WHAT**\n",
    "\n",
    "AdaBoost [Freund and Schapire](#Firstpaper) is a method to produce a powerful ensemble by building a sequence of estimators.\n",
    "\n",
    "**WHY**\n",
    "\n",
    "Theory fundation: Any weak learning algorithm (performs only slightly better than random guessing, error is less than $1/2-\\gamma$) can be efficiently transformed or boosted into a strong learning algorithm \\[[Schaprie](#Schaprie)\\].\n",
    "\n",
    "\n",
    "**HOW**\n",
    "\n",
    "AdaBoost fits a sequence of weak learners on repeatedly modified versions of the data. The data are modified by applying weights $w_1, w_2, ... , w_N$ to each of the training samples in each iteration. \n",
    "\n",
    "Initially, all the samples have the same weights $w_i = 1/N$, and at the first step, a weak learner is trained on the orginal data. \n",
    "\n",
    "Then, in each successive iteration, the sample weights are individually modified and a learner is trained based on the reweighted data. At a given step, the wights of samples that were incorrectly predicted by the boosted model induced at the previous step are increased, while for the samples that were predicted correctly, their weights are decreased.\n",
    "\n",
    "As iterations proceed, examples that are hard to predict will receive very high weights.\n",
    "\n",
    "<a name=\"Firstpaper\"></a> \\[[Freund and Schapire](https://books.google.com.au/books?hl=en&lr=&id=CAnxFA6DaagC&oi=fnd&pg=PA23&dq=a+decision+theoretic+generalization+of+online+learning&ots=XaVCZZZXsD&sig=2YyHZOvGolS_gG7uhZ1KuPZYjPc#v=onepage&q=a%20decision%20theoretic%20generalization%20of%20online%20learning&f=false)\\] Schapire, R.E. and Freund, Y., 1995, March. A decision-theoretic generalization of on-line learning and an application to boosting. In Second European Conference on Computational Learning Theory (pp. 23-37).\n",
    "\n",
    "<a name=\"Schapire\"></a> \\[[Schapire](https://cs.rochester.edu/u/stefanko/Teaching/09CS446/Boosting.pdf)\\] Schapire, R.E., 1990. The strength of weak learnability. Machine learning, 5(2), pp.197-227."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 1995 paper\n",
    "\n",
    "#### 1.  1st reading\n",
    "##### 1.1 Understanding\n",
    "1. At a given iteration t, the output is made only based on the the specific hypothesis $h_t$\n",
    "<img src=\"images/AdaBoost_process.jpg\" width=\"600\"/>\n",
    "2. The details and formulas\n",
    "<img src=\"images/AdaBoost_Alg.jpg\" width=\"600\"/>\n",
    "\n",
    "##### 1.2 Questions \n",
    "1. For the calculation of $\\beta_t$, some have a coefficient 1/2, while some don't have. What does 1/2 do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a AdaBoost algorithm step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. let's define a weak learner: 1-depth decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_tree = DecisionTreeClassifier(max_depth = 1, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Implement the AdaBoost\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def myAdaBoost(X_train, y_train, X_test, y_test, weak_learner, m_iteration=200):\n",
    "    # 1. initialise weights for each of the N examples, w_i=1/N\n",
    "    N, N_test = len(X_train), len(X_test)\n",
    "    w = np.ones(X_train.shape[0])/N\n",
    "    \n",
    "    # 2. boosting\n",
    "    \n",
    "    predictions, test_predictions = [], []\n",
    "    model_weights = []\n",
    "    \n",
    "    for m in range(1, m_iteration+1):\n",
    "        # (a). train a weak classfier G_m(x) to training data with weights w\n",
    "        weak_learner.fit(X_train, y_train, sample_weight=w)\n",
    "        \n",
    "        # wls.append(weak_learner)  # !!! model append in loop does not work?!!!\n",
    "        \n",
    "        # (b). calculate error for G_m(x)\n",
    "        y_pre = weak_learner.predict(X_train)\n",
    "        predictions.append(y_pre)\n",
    "        error = np.sum(w*1*(y_pre != y_train)) / np.sum(w)\n",
    "        \n",
    "        # for testing\n",
    "        y_test_pre = weak_learner.predict(X_test)\n",
    "        test_predictions.append(y_test_pre)\n",
    "        \n",
    "        # (c). compute alpha_m\n",
    "        alpha_m = np.log((1-error)/error)\n",
    "        model_weights.append(alpha_m)\n",
    "        \n",
    "        # (d). update training examples' weights\n",
    "        w = w * np.exp(alpha_m*1*(y_pre != y_train))\n",
    "        \n",
    "    # 3. get the weighted majority vote\n",
    "    model_weights = np.array(model_weights)  # m\n",
    "    print('model weights: ', model_weights)\n",
    "    \n",
    "    \n",
    "    # training\n",
    "    predictions = np.array(predictions) # m*N, predict value is either 1 or -1\n",
    "    train_final = np.sign(np.sum(np.transpose(predictions)*model_weights, axis=1))\n",
    "    print('Train accuracy', np.sum(y_train==train_final)/N)\n",
    "    \n",
    "    \n",
    "    # testing\n",
    "    test_predictions = np.array(test_predictions)\n",
    "    test_final = np.sign(np.sum(np.transpose(test_predictions)*model_weights, axis=1))\n",
    "    print('Test accuracy', np.sum(y_test==test_final)/N_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model weights:  [0.84729786 1.29928298 1.5040774 ]\n",
      "Train accuracy 0.6\n",
      "Test accuracy 0.5\n"
     ]
    }
   ],
   "source": [
    "X_train = np.arange(10).reshape(10,1)\n",
    "y_train = np.array([1,1,1,-1,-1,-1,1,1,1,-1])\n",
    "\n",
    "X_test = np.array([5.8, 2.1, 4.6, 7.9]).reshape(4,1)\n",
    "y_test = np.array([-1, 1, -1, 1])\n",
    "\n",
    "myAdaBoost(X_train, y_train, X_test, y_test, clf_tree, m_iteration=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.array([0.1, 0.3, 0.8])\n",
    "print(alphas)\n",
    "print(predictions)\n",
    "print(np.transpose(predictions)*alphas)\n",
    "np.sign(np.sum(np.transpose(predictions)*alphas, axis=1))\n",
    "predictions==y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Two-class AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_gaussian_quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, y1 = make_gaussian_quantiles(cov=2., n_samples=200, n_features=2,\n",
    "                                n_classes=2, random_state=1)\n",
    "X2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5,\n",
    "                                 n_samples=300, n_features=2,\n",
    "                                 n_classes=2, random_state=1)\n",
    "X = np.concatenate((X1, X2))\n",
    "y = np.concatenate((y1, -y2+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(list(y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AdaBoosted DT\n",
    "bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), algorithm=\"SAMME\",n_estimators=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit\n",
    "bdt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean accuracy of self.predict(X) wrt. y.\n",
    "bdt.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = len(bdt)\n",
    "\n",
    "# error at each iteration\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(range(1, n_trees + 1), bdt.estimator_errors_[:n_trees],\n",
    "         \"b\", label='SAMME', alpha=.5)\n",
    "plt.legend()\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Number of Trees')\n",
    "\n",
    "# boost weight of each tree\n",
    "plt.subplot(122)\n",
    "plt.plot(range(1, n_trees + 1), bdt.estimator_weights_[:n_trees],\n",
    "         \"b\", label='SAMME', alpha=.5)\n",
    "plt.legend()\n",
    "plt.ylabel('Weight')\n",
    "plt.xlabel('Number of Trees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_colors = \"br\"\n",
    "plot_step = 0.02\n",
    "class_names = \"AB\"\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "# Plot the decision boundaries\n",
    "# plt.subplot(121)\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                     np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "Z = bdt.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "plt.axis(\"tight\")\n",
    "\n",
    "# Plot the training points\n",
    "for i, n, c in zip(range(2), class_names, plot_colors):\n",
    "    idx = np.where(y == i)\n",
    "    plt.scatter(X[idx, 0], X[idx, 1],\n",
    "                c=c, cmap=plt.cm.Paired,\n",
    "                s=20, edgecolor='k',\n",
    "                label=\"Class %s\" % n)\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Decision Boundary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the two-class decision scores\n",
    "twoclass_output = bdt.decision_function(X)\n",
    "plot_range = (twoclass_output.min(), twoclass_output.max())\n",
    "plt.figure(figsize=(5,5))\n",
    "for i, n, c in zip(range(2), class_names, plot_colors):\n",
    "    plt.hist(twoclass_output[y == i],\n",
    "             bins=10,\n",
    "             range=plot_range,\n",
    "             facecolor=c,\n",
    "             label='Class %s' % n,\n",
    "             alpha=.5,\n",
    "             edgecolor='k')\n",
    "x1, x2, y1, y2 = plt.axis()\n",
    "plt.axis((x1, x2, y1, y2 * 1.2))\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Samples')\n",
    "plt.xlabel('Score')\n",
    "plt.title('Decision Scores')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-4-prac]",
   "language": "python",
   "name": "conda-env-py3-4-prac-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
