{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "- Usage: classification\n",
    "- Generative (probabilistic) model\n",
    "- Naive: each input variable is independent from each other\n",
    "- Effectiveness: \n",
    "> Despite these rather optimistic assumptions, naive Bayes classifiers often outperform far more sophisticated alternatives. The reasons are related to Figure 6.15: although the individual class density estimates may be biased, this bias might not hurt the posterior probabilities as much, especially near the decision regions. In fact, the problem may be able to withstand considerable bias for the savings in variance such a “naive” assumption earns. (ESL pp.211)\n",
    "\n",
    "<img src='images/NaiveBayes_effectiveness.png' width='600'>\n",
    "\n",
    "> In spite of their apparently over-simplified assumptions, naive Bayes classifiers have worked quite well in many real-world situations, <u>famously document classification and spam filtering</u>. They require a small amount of training data to estimate the necessary parameters. ([sklearn documentation](https://scikit-learn.org/stable/modules/naive_bayes.html))\n",
    "\n",
    "- Efficiency:\n",
    "> The naive Bayes classifier **can be extremely fast** compared to more sophisticated methods. The decoupling of the class conditional feature distributions means that <u>each distribution can be independently estimated as a one dimensional distribution</u>. This in turn helps to alleviate problems stemming from the curse of dimensionality. ([sklearn documentation](https://scikit-learn.org/stable/modules/naive_bayes.html))\n",
    "\n",
    "\n",
    "### `Algorithm Overview`\n",
    "**Input** \n",
    "- Training data $T=\\{(x_1, y_1),...(x_N,y_N)\\}$, where $x_i \\in \\mathcal{X} \\subseteq \\mathbb{R}^n$, $x_i=(x_i^{(1)}, x_i^{(2)}, \\ldots, x_i^{(n)})$ and $y_i \\in \\mathcal{y} = {c_1,c_2,...c_K}$ are the classes. And for a feature $x_i^{(j)}$, its possible value set is $x_i^{(j)} \\in \\{a_{j1}, a_{j2}, ... , a_{jS_j}\\}$. For an exmaple, $a_{jl}$ is the value of $j^{th}$ featue may be the $l^{th}$, $l \\in \\{1,2,3, \\ldots, S_j\\}$\n",
    "- Input example $x$\n",
    "\n",
    "**Output**\n",
    "- The class that $x$ belongs to.\n",
    "\n",
    "**Algorithm**\n",
    "- Goal: maximise the posterior probabilities $\\max\\limits_{c_k}P(y=c_k|x)$.\n",
    "    * It's proven that maximising the posterior probabilitie equals to minimising the 0-1 loss. \n",
    "- Bayes equation:\n",
    "$$\\max\\limits_{c_k}P(y=c_k|x) = \\max\\limits_{c_k}\\frac{P(x|y=c_k)P(y=c_k)}{P(x)}$$\n",
    "For an exmaple $x$, $P(x)$ is same for any class $c_i$. So the problem of finding the maximum posterior probability equals to $$\\max\\limits_{c_k}P(x|y=c_k)P(y=c_k)$$\n",
    "where $P(x|y=c_k)$ is the conditional probability, and $P(y=c_k)$ is the prior probability.\n",
    "\n",
    "- Naive: strong assumption on the features: given a class, the features $x^{(j)}$ are independent: \n",
    "$$P(x)=\\prod_j^nP(x^{(j)})$$\n",
    "$$P(x|y)=P(x^{(1)}|y) \\cdot P(x^{(2)}|y) \\ldots \\cdot P(x^{(n)}|y)=\\prod_j^nP(x^{(j)}|y)$$\n",
    "Under this naive assumption, the problem now is to solve:\n",
    "\\begin{align}\n",
    "&\\max\\limits_{c_k}P(x|y=c_k)P(y=c_k) \\\\ = &\\max\\limits_{c_k}P(y=c_k)\\prod_j^nP(x^{(j)}|y=c_k) \\label{eq:goal} \\tag{3}\n",
    "\\end{align}\n",
    "\n",
    "- How to estimate $P(x|y=c_k)$ and $P(y=c_k)$?\n",
    "    * Estimate $P(y=c_k)$ \n",
    "        1. Maximum Likelihood Estimator\n",
    "            $$P(y=c_k) = \\frac{\\sum_{i=1}^N I(y_i=c_k)}{N} = \\frac{N_y}{N}\\label{eq:pp} \\tag{2}$$\n",
    "            $$\\quad k=1,2,\\ldots,K; \\quad i=1,2,\\ldots,N$$\n",
    "        2. Maximum A Posteriori (MAP):same as Equation $\\eqref{eq:pp}$\n",
    "    * Estimate $P(x|y=c_k)$\n",
    "        1. When the input variable is discrete: maximum likelihood estimator\n",
    "            \\begin{align}\n",
    "            P(x^{(j)}=a_{jl}|y=c_k) &= \\prod_j^nP(x^{(j)}|y=c_k) \\\\\n",
    "            &= \\prod_j^n\\frac{\\sum_{i=1}^N I\\Big((x_i^{(j)}=a_{jl}) \\quad \\&\\& \\quad (y_i=c_k)\\Big)}{\\sum_{i=1}^N I(y_i=c_k)} \\\\\n",
    "            &= \\prod_j^n\\frac{N_{yj}}{N_y}\n",
    "            \\label{eq:cp} \\tag{1}\n",
    "            \\end{align}\n",
    "            $$j = 1,2,\\ldots,n; \\quad l=1,2,\\ldots,S_j; \\quad k=1,2,\\ldots,K; \\quad i=1,2,\\ldots,N$$\n",
    "        2. when the input varibale is continuous:\n",
    "            * continuous $\\rightarrow$ discrete\n",
    "            * assume a distribution (e.g., Gaussian), and fit a probability density function (pdf)\n",
    "               \n",
    "- **Avoid Numerical Underflow with Log**: the multiplication of many probabilities together can become numerically unstable, especially when the dimension of input features is high. To overcome this problem, it is common to change the calculation from the product of probabilities to the sum of log probabilities: $\\log(P(x^{(1)}|y=c_k)) + \\log(P(x^{(2)}|y=c_k)) \\ldots +  \\log(P(x^{(n)}|y=c_k))$\n",
    "\n",
    "- **Prevent zero probablities in further computations**: smoothing. For a class $y$, $θ_{yi}$ is the probability \n",
    "$P(x_i)∣y)$of feature $i$ appearing in a sample belonging to class $y$. \n",
    "$$\\hat{\\theta}_{yi}=\\frac{N_{yj}+\\alpha}{N_y+\\alpha n}$$\n",
    "The smoothing priors $\\alpha \\leq 0$ accounts for features not present in the learning samples and prevents zero probabilities in further computations. Setting $\\alpha=1$ is called **Laplace smoothing**, while $\\alpha < 1$ is called **Lidstone smoothing**.\n",
    "\n",
    "- Variations: different naive Bayes classifier differ mainly by the assumptions they make regarding the distribution of $P(x^{(j)}|y=c_k)$\n",
    "    * **Multinomial Naive Bayes** Hi\n",
    "    \n",
    "    * **Gaussian Naive Bayes**: The likelihood of the features is assumed to be Gaussian.\n",
    "        $$P(x^{(j)}|y) = \\frac{1}{\\sqrt{2\\pi (\\sigma_y^{(j)})^2}} exp\\Big(-\\frac{(x_i- \\mu_y^{(j)})^2}{2(\\sigma_y^{(j)})^2} \\Big)$$\n",
    "        For a class $y$, $\\mu_y^{(j)}$,$\\sigma_y^{(j)}$ can be estimated as the mean, variance of all the training examples in terms of feature $j$.\n",
    "        \n",
    "    * **Complement Naive Bayes**\n",
    "    \n",
    "    * **Bernoulli Naive Bayes**\n",
    "    \n",
    "- For $c_k$ classes and $n$ input features, navie Bayes computes $c_k$ prior probabilities.  \n",
    "- For $c_k$ classes and $n$ input features, navie Bayes computes ? conditional probabilities?\n",
    "    * If all the features are categorical, $c_k \\sum_{j=1}^nS_j$ conditional probabilities are computed\n",
    "    * In general, $c_k*n$ different **probability distributions** must be created and maintained\n",
    "\n",
    "- **Steps:**\n",
    "    1. compute conditional probability with Equation $\\eqref{eq:cp}$; compute prior probability with Equation $\\eqref{eq:pp}$\n",
    "    2. Given $x$, compute $P(y=c_k)\\prod_j^nP(x^{(j)}|y=c_k)$ under each $c_k$\n",
    "    3. Select the class with the largest posterior probability as the classification for the given instance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.11问题：\n",
    "1. 如果input features中既有取值仅有几类的离散变量，又有符合高斯分布的变量，又有符合其他分布的变量，怎么确定对哪个类别用哪类方法估计$p(x|y=c_k)$？ How to deal with data set containing numeric variables and category variables together? (2019.11.11)\n",
    "    \n",
    "    Good question. (Here)[https://stackoverflow.com/questions/14254203/mixing-categorial-and-continuous-data-in-naive-bayes-classifier-using-scikit-lea?rq=1] are two options: 1. transform all to categorical 2. train naive Bayes separately, and then refit a new model based on the predict probabilities.\n",
    "    \n",
    "    \n",
    "2. Gaussian naive Bayes中，如何fit每个输入变量的Gaussian distribution？又如何计算要预测的数据的概率密度？(2019.11.11)\n",
    "\n",
    "    Answer: For Gaussian distribution, it is determined when two variables: mean and variance are determined. So we need to estimate these two variables. $\\mu$ is the sample mean and $\\sigma^2$ is the sample variance. Then for an input $x$, we can just calculate the corresponding probability density. (2019.11.12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 150 points : 6\n",
      "{'priors': None, 'var_smoothing': 1e-09}\n",
      "[[5.006 3.428 1.462 0.246]\n",
      " [5.936 2.77  4.26  1.326]\n",
      " [6.588 2.974 5.552 2.026]]\n",
      "[[0.121764 0.140816 0.029556 0.010884]\n",
      " [0.261104 0.0965   0.2164   0.038324]\n",
      " [0.396256 0.101924 0.298496 0.073924]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "\n",
    "y_pred = gnb.fit(iris.data, iris.target).predict(iris.data)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (iris.data.shape[0],(iris.target != y_pred).sum()))\n",
    "\n",
    "print(gnb.get_params())\n",
    "\n",
    "print(gnb.theta_)\n",
    "print(gnb.sigma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation**\n",
    "\n",
    "Example:\n",
    "\n",
    "`X_train = [[1, 'S'], [1, 'M'], [1, 'M'], [1, 'S'],  [1, 'S'],\n",
    "               [2, 'S'], [2, 'M'], [2, 'M'], [2, 'L'],  [2, 'L'],\n",
    "               [3, 'L'], [3, 'M'], [3, 'M'], [3, 'L'],  [3, 'L']]`\n",
    "               \n",
    "`Y_train = [-1, -1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1] `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (15, 2) ; y_train (15,)\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "def normalize(X, axis=-1, order=2):\n",
    "    \"\"\" Normalize the dataset X \"\"\"\n",
    "    l2 = np.atleast_1d(np.linalg.norm(X, order, axis))\n",
    "    l2[l2 == 0] = 1\n",
    "    return X / np.expand_dims(l2, axis)\n",
    "\n",
    "\n",
    "def shuffle_data(X, y, seed=None):\n",
    "    \"\"\" Random shuffle of the samples in X and y \"\"\"\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "    idx = np.arange(X.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "\n",
    "def train_test_split(X, y, test_size=0.5, shuffle=True, seed=None):\n",
    "    \"\"\" Split the data into train and test sets \"\"\"\n",
    "    if shuffle:\n",
    "        X, y = shuffle_data(X, y, seed)\n",
    "    # Split the training data from test data in the ratio specified in\n",
    "    # test_size\n",
    "    split_i = len(y) - int(len(y) // (1 / test_size))\n",
    "    X_train, X_test = X[:split_i], X[split_i:]\n",
    "    y_train, y_test = y[:split_i], y[split_i:]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\" Compare y_true to y_pred and return the accuracy \"\"\"\n",
    "    accuracy = np.sum(y_true == y_pred, axis=0) / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class NaiveBayes():\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.classes = np.unique(y)\n",
    "        self.parameters = {}\n",
    "        for i, c in enumerate(self.classes):\n",
    "            # 计算每个种类的平均值，方差，先验概率\n",
    "            X_Index_c = X[np.where(y == c)]\n",
    "            X_index_c_mean = np.mean(X_Index_c, axis=0, keepdims=True)\n",
    "            X_index_c_var = np.var(X_Index_c, axis=0, keepdims=True)\n",
    "            parameters = {\"mean\": X_index_c_mean, \"var\": X_index_c_var, \"prior\": X_Index_c.shape[0] / X.shape[0]}\n",
    "            self.parameters[\"class\" + str(c)] = parameters\n",
    "\n",
    "    def _pdf(self, X, classes):\n",
    "        # 一维高斯分布的概率密度函数\n",
    "        # eps为防止分母为0\n",
    "        eps = 1e-4\n",
    "        mean = self.parameters[\"class\" + str(classes)][\"mean\"]\n",
    "        var = self.parameters[\"class\" + str(classes)][\"var\"]\n",
    "\n",
    "        # 取对数防止数值溢出\n",
    "        # numerator.shape = [m_sample,feature]\n",
    "        numerator = np.exp(-(X - mean) ** 2 / (2 * var + eps))\n",
    "        denominator = np.sqrt(2 * np.pi * var + eps)\n",
    "\n",
    "        # 朴素贝叶斯假设(每个特征之间相互独立)\n",
    "        # P(x1,x2,x3|Y) = P(x1|Y)*P(x2|Y)*P(x3|Y),取对数相乘变为相加\n",
    "        # result.shape = [m_sample,1]\n",
    "        result = np.sum(np.log(numerator / denominator), axis=1, keepdims=True)\n",
    "\n",
    "        return result.T\n",
    "\n",
    "    def _predict(self, X):\n",
    "        # 计算每个种类的概率P(Y|x1,x2,x3) =  P(Y)*P(x1|Y)*P(x2|Y)*P(x3|Y)\n",
    "        output = []\n",
    "        for y in range(self.classes.shape[0]):\n",
    "            prior = np.log(self.parameters[\"class\" + str(y)][\"prior\"])\n",
    "            posterior = self._pdf(X, y)\n",
    "            prediction = prior + posterior\n",
    "            output.append(prediction)\n",
    "        return output\n",
    "\n",
    "    def predict(self, X):\n",
    "        # 取概率最大的类别返回预测值\n",
    "        output = self._predict(X)\n",
    "        output = np.reshape(output, (self.classes.shape[0], X.shape[0]))\n",
    "        prediction = np.argmax(output, axis=0)\n",
    "        return prediction\n",
    "\n",
    "\n",
    "\n",
    "#主函数\n",
    "def main():\n",
    "    \n",
    "#     data = datasets.load_digits()\n",
    "#     X = normalize(data.data)\n",
    "#     y = data.target\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "#     print(\"X_train\",X_train.shape, \"; y_train\", y_train.shape)\n",
    "    \n",
    "#     X_train = np.array([[1, 'S'], [1, 'M'], [1, 'M'], [1, 'S'],  [1, 'S'],\n",
    "#                [2, 'S'], [2, 'M'], [2, 'M'], [2, 'L'],  [2, 'L'],\n",
    "#                [3, 'L'], [3, 'M'], [3, 'M'], [3, 'L'],  [3, 'L']])\n",
    "    X_train = np.array([[1, 1], [1, 2], [1, 2], [1, 1],  [1, 1],\n",
    "               [2, 1], [2, 2], [2, 2], [2, 3],  [2, 3],\n",
    "               [3, 3], [3, 2], [3, 2], [3, 3],  [3, 3]])\n",
    "    y_train = np.array([2, 2, 1, 1, 0, 2, 0, 1, 1, 1, 1, 1, 1, 1, 0])\n",
    "    \n",
    "    X_test = np.array([[1,1]])\n",
    "    y_test = np.array([2])\n",
    "    print(\"X_train\",X_train.shape, \"; y_train\", y_train.shape)\n",
    "    \n",
    "    clf = NaiveBayes()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print (\"Accuracy:\", accuracy)\n",
    "\n",
    "    # Reduce dimension to two using PCA and plot the results\n",
    "#     Plot().plot_in_2d(X_test, y_pred, title=\"Naive Bayes\", accuracy=accuracy, legend_labels=data.target_names)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Naive Bayes\n",
    "Example: [Using navie Bayes for text classification](https://github.com/szcf-weiya/ESL-CN/blob/master/code/NaiveBayes/python/bayes.py):\n",
    "\n",
    "Inputs are one-hot vector\n",
    "\n",
    "Classes must be 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- vocabulary list is: --- \n",
      " 32\n",
      "---Each doc to vector: --- \n",
      " [[1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1], [0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0], [1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]]\n",
      "(6, 32)\n",
      "32\n",
      "32\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loadDataSet():\n",
    "    postingList = [['my', 'my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1] #1\n",
    "    return postingList, classVec\n",
    "\n",
    "\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document)  # s|t is s.union(t)\n",
    "    return list(vocabSet)\n",
    "\n",
    "# word occurrence vectors (1: occur 0: doesn't occur)\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else: \n",
    "            print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec   \n",
    " \n",
    "    \n",
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    # N examples\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    # n features\n",
    "    numWords = len(trainMatrix[0])\n",
    "    \n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)  # why use sum? classes can only be 0 and 1?\n",
    "    \n",
    "    #p0Num = np.zeros(numWords); p1Num = np.zeros(numWords)\n",
    "    p0Num = np.ones(numWords); p1Num = np.ones(numWords)\n",
    "    \n",
    "    #p0Denom = 0.0; p1Denom = 0.0\n",
    "    p0Denom = 2.0; p1Denom = 2.0\n",
    "    \n",
    "    for i in range(numTrainDocs):\n",
    "        if (trainCategory[i] == 1):\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    \n",
    "    #p1Vect = p1Num/p1Denom\n",
    "    #p0Vect = p0Num/p0Denom\n",
    "    p1Vect = np.log(p1Num/p1Denom)  # p(x^{(j)}/y=1)\n",
    "    p0Vect = np.log(p0Num/p0Denom)  # p(x^{(j)}/y=0)\n",
    "    return p0Vect, p1Vect, pAbusive\n",
    "\n",
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + np.log(pClass1)\n",
    "    p0 = sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "    \n",
    "    listOPosts,listClasses = loadDataSet()\n",
    "    \n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    print('--- vocabulary list is: --- \\n', len(myVocabList))\n",
    "    \n",
    "    # prepare training data\n",
    "    trainMat = []\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    print('---Each doc to vector: --- \\n',trainMat)\n",
    "    print(np.array(trainMat).shape)\n",
    "    \n",
    "    p0V,p1V,pAb=trainNB0(trainMat,listClasses)\n",
    "    print(len(p0V))\n",
    "    print(len(p1V))\n",
    "    print(pAb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-4-prac]",
   "language": "python",
   "name": "conda-env-py3-4-prac-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
