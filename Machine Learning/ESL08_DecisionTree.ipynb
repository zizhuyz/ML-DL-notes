{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "- usage: classfication and regression\n",
    "- Can be regarded as determinstic model or probabilistic model\n",
    "-  A **white box** model: simple to understand and to interpret\n",
    "- non-parametric model: the number of parameters is not fixed\n",
    "- effectiveness:\n",
    "> The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in **an ensemble learner** (random forest), where the features and samples are randomly sampled with replacement. (scikit-learn document)[https://scikit-learn.org/stable/modules/tree.html]\n",
    "- efficiency:\n",
    "    * In general, the run time cost to contruct a balanced binary tree is $O(N*n*\\log(N))$, where $N$ is the number of samples, and $n$ is the number of features.\n",
    "    * The cost of using the tree to make prediction $O(\\log(N))$\n",
    "- Three elements in decision tree:\n",
    "    1. feature selection (ID3, C4.5, CART)\n",
    "    2. tree training/generation (local optimal)\n",
    "    3. tree pruning (global optimal)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3 (Iterative Dichotomiser 3)\n",
    "- [Quinlan 1979](http://hunch.net/~coms-4771/quinlan.pdf)\n",
    "- key idea: information gain of a feature\n",
    "- Entropy $H(X)$: $X$ is a discrete random variable, and its prob. distribution is $P(X=x_i)=p_i, \\quad i=1,2,\\ldots,n$. The entropy of $X$ is: $$H(X) = - \\sum_{i=1}^n p_i \\log p_i$$\n",
    "    A larger entropy means a higher degree of uncertainty.\n",
    "- Conditional Entropy $H(Y|X)$: the expectation of the entropy of conditional prob. $p(Y|X)$ to $X$ ($X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望): $$H(Y|X)=\\sum_{i=1}^n p_i H(Y|X=x_i)$$\n",
    "\n",
    "- **Information gain (IG)**: the IG of a feature $A$ to the training data $D$, denoted as $g(D,A)$, is the difference between $H(D)$ and $H(D|A)$: $$g(D,A) = H(D) - H(D|A) \\label{eq:IG} \\tag{3}$$\n",
    "    IG stands for how much the uncertainty of $D$ has been decreased if we know feature $A$.\n",
    "    \n",
    "    IG is also the mutual information between the output class (Y) and an input feature.\n",
    "    \n",
    "    $D$ is the training data. $|D|$ is $n_{samples}$. $K$ is the number of classes. $|C_k|$ is the number of samples which belong to class $k$. Suppose a discrete feature $A$ with possible values $\\{a_1, a_2, \\ldots, a_n\\}$. Based on the values of $A$, $D$ is partitioned into $n$ subsets $\\{D_1, D_2, \\ldots, D_n\\}$. $|D_{ik}|$ is the number of examples in subset $D_i$ and belonging to class $k$.\n",
    "    $$H(D) = - \\sum_{k=1}^K \\frac{|C_k|}{|D|} \\log \\frac{|C_k|}{|D|} \\tag{1}$$\n",
    "    \\begin{align}\n",
    "    H(D|A) &= \\sum_{i=1}^n \\frac{|D_i|}{|D|} H(D|A=a_i) \\\\ &= -\\sum_{i=1}^n \\frac{|D_i|}{|D|} \\sum_{k=1}^K \\frac{|D_{ik}|}{|D_i|} \\log \\frac{|D_{ik}|}{|D_i|} \\tag{2}\n",
    "    \\end{align}\n",
    "- Dealing with continuous input features: use several intervals to partition continuous variable.\n",
    "    \n",
    "- **ID3 Algorithm to generate Tree $T$**\n",
    "\n",
    "    Input: labelled traing data $D$\n",
    "    \n",
    "    Output: Tree $T$\n",
    "    \n",
    "    1. Create a node $N$\n",
    "    2. If all the training exmaples belong to a same class, $N$ is the only node in $T$ and label $N$ with the calss. Return $T$ as $N$.\n",
    "    3. If there are zero input features, $N$ is the only node in $T$ and label $N$ with the majority calss. Return $T$ as $N$.\n",
    "    4. Otherwise, choose the feature $A$ with the maximum $g(D,A)$ as the partition feature of the current node $N$\n",
    "    5. For each value $a_i$ in $A$, create a branch from node $N$ with a sub training dataset $D_i$. If $D_i$ is not empty, repeat 4 and 5.\n",
    "\n",
    "- ID3 limitation: tend to choose features who have a larger number of values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C4.5\n",
    "- [Quinlan, 1993](https://en.wikipedia.org/wiki/C4.5_algorithm#cite_note-1)\n",
    "- improved ID3: \n",
    "    1. use IG ratio to choose features\n",
    "     $$g_R(D,A) = \\frac{H(D) - H(D|A)}{H_A(D)} \\label{eq:IGR} \\tag{4}$$\n",
    "     where $H_A(D) = -\\sum_{i=1}^n \\frac{|D_i|}{|D|} \\log \\frac{|D_i|}{|D|}$, $n$ is the number of different values that feature $A$ has.\n",
    "    2. handling both continuous and discrete features\n",
    "    3. training data with missing attribute values\n",
    "    4. pruning trees after creation\n",
    "> A landmark decision tree program that is probably the machine learning workhorse most widely used in practice to date. (2011, authors of the Weka machine learning software)\n",
    "\n",
    "### C5.0\n",
    "- Quinlan\n",
    "- both [public source code](https://www.rulequest.com/see5-unix.html) and commercial version are available\n",
    "- [A number of improvements on C4.5](https://www.rulequest.com/see5-comparison.html)\n",
    "    1. faster\n",
    "    2. smaller\n",
    "    3. boosting (more accurate)\n",
    "    4. weighting (allows you to weight different cases and misclassification types)\n",
    "    5. less memory usage\n",
    "    6. winnowing: winnows the attributes to remove those that may be unhelpful\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ID3 Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy\n",
    "# ig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning Tree\n",
    "- objective: minimise cost \n",
    "    \\begin{align}\n",
    "       C_\\alpha(T) &= C(T)+\\alpha|T| \\\\\n",
    "                   &= \\sum_{t=1}^{|T|}|N_t| H_t(T)+\\alpha|T| \\\\\n",
    "                   &= -\\sum_{t=1}^{|T|}|N_t| \\sum_{k=1}^{K} \\frac{|N_{tk}|}{|N_t|} \\log \\frac{|N_{tk}|}{|N_t|}+\\alpha|T| \\\\\n",
    "                   &= -\\sum_{t=1}^{|T|}\\sum_{k=1}^{K} |N_{tk}|\\log \\frac{|N_{tk}|}{|N_t|}+\\alpha|T|\n",
    "                   \\tag{4}\n",
    "    \\end{align}\n",
    "    where $|T|$ is the number of leaf nodes in $T$. For a leaf node $t$, the number of samples in it is $N_t$, and $N_{tk}$ is the number of sample in node $t$ and belongs to class $k$.\n",
    "    \n",
    "    $C(T)$ is the training error, and $|T|$ stands for the model complexity. $\\alpha$ is used to seek a balance between training error and model complexity; a larger $\\alpha$ means that we want a more simple model (less leaf nodes). When $\\alpha=0$, the optimal tree is the original tree $T$ itself.\n",
    "\n",
    "- idea: compare the costs between the current tree $T_A$ with the tree $T_B$ which is a part of $T_A$ by cutting some leaf nodes in $T_A$: if $C_\\alpha(T_A) \\geq C_\\alpha(T_B)$, then go with prune.\n",
    "\n",
    "<img src='images/DT_pruning.png' width='700'>\n",
    "\n",
    "- **Pruning algorithm for ID3, C4.5**\n",
    "\n",
    "    Input: Tree $T$ and parameter $\\alpha$\n",
    "    \n",
    "    Output: Tree after pruning $T_\\alpha$ \n",
    "    \n",
    "    1. calculate $H_t(T)$ for every node in the Tree $T$\n",
    "    2. recursively cut from leaf nodes. Suppose $C_\\alpha(T_A)$ and $C_\\alpha(T_B)$ are the costs before and after pruning respectively, if $C_\\alpha(T_A) \\geq C_\\alpha(T_B)$, then go with prune.\n",
    "    3. Repeat 2 until stop, then we can get the subtree $T_\\alpha$ whose cost function is the minimum.\n",
    "    \n",
    "    limitation: $\\alpha$ is decided based on experience."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CART(Classification and Regression Trees)\n",
    "- L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-4-prac]",
   "language": "python",
   "name": "conda-env-py3-4-prac-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
