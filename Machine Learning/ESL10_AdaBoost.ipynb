{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "Ensemble methods:\n",
    "<img src=\"images/ensemble_outline.jpg\" width=\"500\"/>\n",
    "\n",
    "**WHAT**\n",
    "\n",
    "AdaBoost [Freund and Schapire](#Firstpaper) is a method to produce a powerful ensemble by building a sequence of estimators.\n",
    "\n",
    "**WHY**\n",
    "\n",
    "Theory fundation: Any weak learning algorithm (performs only slightly better than random guessing, error is less than $1/2-\\gamma$) can be efficiently transformed or boosted into a strong learning algorithm \\[[Schaprie](#Schaprie)\\].\n",
    "\n",
    "\n",
    "**HOW**\n",
    "\n",
    "AdaBoost fits a sequence of weak learners on repeatedly modified versions of the data. The data are modified by applying weights $w_1, w_2, ... , w_N$ to each of the training samples in each iteration. \n",
    "\n",
    "Initially, all the samples have the same weights $w_i = 1/N$, and at the first step, a weak learner is trained on the orginal data. \n",
    "\n",
    "Then, in each successive iteration, the sample weights are individually modified and a learner is trained based on the reweighted data. At a given step, the wights of samples that were incorrectly predicted by the boosted model induced at the previous step are increased, while for the samples that were predicted correctly, their weights are decreased.\n",
    "\n",
    "As iterations proceed, examples that are hard to predict will receive very high weights.\n",
    "\n",
    "<a name=\"Firstpaper\"></a> \\[[Freund and Schapire](https://books.google.com.au/books?hl=en&lr=&id=CAnxFA6DaagC&oi=fnd&pg=PA23&dq=a+decision+theoretic+generalization+of+online+learning&ots=XaVCZZZXsD&sig=2YyHZOvGolS_gG7uhZ1KuPZYjPc#v=onepage&q=a%20decision%20theoretic%20generalization%20of%20online%20learning&f=false)\\] Schapire, R.E. and Freund, Y., 1995, March. A decision-theoretic generalization of on-line learning and an application to boosting. In Second European Conference on Computational Learning Theory (pp. 23-37).\n",
    "\n",
    "<a name=\"Schapire\"></a> \\[[Schapire](https://cs.rochester.edu/u/stefanko/Teaching/09CS446/Boosting.pdf)\\] Schapire, R.E., 1990. The strength of weak learnability. Machine learning, 5(2), pp.197-227."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 1995 paper\n",
    "\n",
    "#### 1.  1st reading\n",
    "##### 1.1 Understanding\n",
    "1. AdaBoost was originally motivated from a very different perspective (an application of online allocation) than the present most discussed version (forward stagewise additive modeling based on exponential loss). \n",
    "2. At a given iteration t, the output is made only based on the the specific hypothesis $h_t$\n",
    "<img src=\"images/AdaBoost_process.jpg\" width=\"600\"/>\n",
    "3. The details and formulas\n",
    "<img src=\"images/AdaBoost_Alg.jpg\" width=\"600\"/>\n",
    "\n",
    "##### 1.2 Questions \n",
    "1. For the calculation of $\\beta_t$, some have a coefficient 1/2, while some don't have. What does 1/2 do?\n",
    "    \n",
    "    这里1/2是使loss function最小化的求解过程中得到的解：\n",
    "    $\\beta_t = \\frac{1}{2}\\log\\frac{1-\\epsilon_t}{\\epsilon_t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost in ESL\n",
    "<img src='images/AdaBoost_M1.png' width='600'>\n",
    "\n",
    "**Additive Expansion**\n",
    "- General form:\n",
    "\\begin{align*}\n",
    "f_m(x) &=  f_{m-1}(x) + \\beta_m b(x; \\gamma_m) \\\\\n",
    "f(x) &=  \\sum_{m=1}^M \\beta_m b(x; \\gamma_m)\n",
    "\\end{align*}\n",
    "\n",
    "- In AdaBoost:\n",
    "\\begin{equation*}\n",
    "f_m(x) = f_{m-1}(x) + \\beta_m G_m(x)\n",
    "\\end{equation*}\n",
    "where $G_m(x) \\in \\{-1,1\\}$.\n",
    "\n",
    "**AdaBoost is a forward stagewise modelling based on exponential loss**\n",
    "\n",
    "- The Exponential Loss Function:\n",
    "    \\begin{equation*}\n",
    "    L(y_i, \\hat{f}(x_i) = exp(-y_i \\cdot \\hat{f}(x_i))\n",
    "    \\label{eq:exp_loss} \\tag{1}\n",
    "    \\end{equation*}\n",
    "    where $y_i \\in \\{-1,1\\}$.\n",
    "\n",
    "- We want to minimise the loss for all examples and all basis estimators (*Attention to the equation here!*)\n",
    "    \\begin{equation*}\n",
    "    \\min\\limits_{\\{\\beta_m, G_m(x)\\}_1^m} \\sum_{i=1}^N L \\Big(y_i, \\sum_{m=1}^M \\beta_m G_m(x_i) \\Big)\n",
    "    \\label{eq:c_el} \\tag{2}\n",
    "    \\end{equation*}\n",
    "\n",
    "- To minimise **Eq. $\\eqref{eq:c_el}$** is difficult, and we then solve a simple variation of it\n",
    "    \\begin{equation*}\n",
    "    \\min\\limits_{\\beta_m, G_m(x)} \\sum_{i=1}^N L(y_i, \\hat{f}_m(x_i) \\\\ =  \\min\\limits_{\\beta_m, G_m(x)} \\sum_{i=1}^N e^{-y_i \\cdot \\hat{f}_m(x_i)}  \n",
    "    \\label{eq:s_el} \\tag{3}\n",
    "    \\end{equation*}\n",
    "\n",
    "<img src='images/AdaBoost_Forward_Stagewise.png' width='600'>\n",
    "\n",
    "- Let's solve **Eq. $\\eqref{eq:s_el}$**. It can be written as:\n",
    "    \\begin{align}\n",
    "    \\sum_{i=1}^N L(y_i, f_m(x_i) &= \\sum_{i=1}^N e^{-y_i(f_{m-1}(x_i) + \\beta_m G_m(x_i))} \\\\\n",
    "    &= \\sum_{i=1}^N e^{-y_if_{m-1}(x_i)} \\cdot e^{-y_i\\beta_m G_m(x_i)} \\\\\n",
    "    &= \\sum_{i=1}^N w_i^{(m)} \\cdot e^{-\\beta_m y_i G_m(x_i)}\n",
    "    \\label{eq:4} \\tag{4}\n",
    "    \\end{align}\n",
    "    \n",
    "    In Eq. $\\eqref{eq:4}$, line 2, we can easily see that the left part is neither related to $\\beta_m$ nor $G_m(x)$, thus we regard it as a \"coefficient\": $w_i^{(m)}=e^{-y_if_{m-1}(x_i)}$ \n",
    "\n",
    "- The solution to **Eq. $\\eqref{eq:4}$** is two steps. For any $\\beta_m > 0$, if we have the most $y_i=G_m(x_i)$, which means $y_i G_m(x_i)=1$, **Eq. $\\eqref{eq:4}$** would be smaller than when there are many $y_i \\neq G_m(x_i)$ Thus, first, we want as many as $y_i=G_m(x_i)$, which means minimise the loss of $G_m(x)$, then based on the results of $G_m(x)$, calculate $\\beta_m$.\n",
    "\n",
    "- **First step: optimal $G_m(x)$** <br/> \n",
    "    The optimal $G_m(x)$ meets:\n",
    "    \\begin{equation*}\n",
    "    \\min\\limits_{G_m(x)} \n",
    "    =\\frac{\\sum_{i=1}^N w_i^{(m)} I(y_i \\neq G_m(x_i))} {\\sum_{i=1}^N w_i^{(m)}}\n",
    "    = \\frac{\\sum_{y_i \\neq G_m(x_i)} w_i^{(m)}}{\\sum_{i=1}^N w_i^{(m)}}\n",
    "    \\end{equation*}\n",
    "    \n",
    "    Let the prediciton error of $G_m(x)$ be $err_m$, we have:\n",
    "    \\begin{equation*}\n",
    "    err_m = \\frac{\\sum_{y_i \\neq G_m(x_i)} w_i^{(m)}}{\\sum_{i=1}^N w_i^{(m)}}\n",
    "    \\label{eq:5} \\tag{5}\n",
    "    \\end{equation*}\n",
    "\n",
    "- **Second step: optimal $\\beta_m$** <br/> \n",
    "    Given this $err_m$, we want to solve $\\beta_m$ in **Eq. $\\eqref{eq:4}$**\n",
    "    \\begin{align}\n",
    "    Eq.(4) = \\sum_{i=1}^N w_i^{(m)} \\cdot e^{-\\beta_m y_i G_m(x_i)} &= \\sum_{y_i=G_m(x_i)} w_i^{(m)} \\cdot e^{-\\beta_m} + \\sum_{y_i \\neq G_m(x_i)} w_i^{(m)} \\cdot e^{\\beta_m} \\\\\n",
    "    &= [\\sum_{i=1}^N w_i^{(m)} \\cdot e^{-\\beta_m} - \\sum_{y_i \\neq G_m(x_i)} w_i^{(m)} \\cdot e^{-\\beta_m}] + \\sum_{y_i \\neq G_m(x_i)} w_i^{(m)} \\cdot e^{\\beta_m}\\\\\n",
    "    & = e^{-\\beta_m} \\sum_{i=1}^N w_i^{(m)}  + (e^{\\beta_m}-e^{-\\beta_m})\\sum_{y_i \\neq G_m(x_i)} w_i^{(m)}\n",
    "    \\label{eq:6} \\tag{6}\n",
    "    \\end{align}\n",
    "    \n",
    "    Then we want to minimise $\\eqref{eq:6}$. Divide it by $\\sum_{i=1}^N w_i^{(m)}$, we get\n",
    "    \\begin{equation*}\n",
    "    e^{-\\beta_m} + (e^{\\beta_m}-e^{-\\beta_m})err_m\n",
    "    \\label{eq:7} \\tag{7}\n",
    "    \\end{equation*}\n",
    "    It's easy to solve $\\eqref{eq:7}$, and we get\n",
    "    \\begin{equation*}\n",
    "    \\beta_m = \\frac{1}{2} \\log\\frac{1-err_m}{err_m}\n",
    "    \\label{eq:8} \\tag{8}\n",
    "    \\end{equation*}\n",
    "\n",
    "- To now, we get the optimal $G_m(x)$ and $\\beta_m$ for **Eq. $\\eqref{eq:s_el}$**. Where $G_m(x)$ is the best model at current iteration $m$ given the inputs at interation $m$. Let $err_m$ be the error of $G_m(x)$, $\\beta_m = \\frac{1}{2} \\log\\frac{1-err_m}{err_m}$.\n",
    "\n",
    "**AdaBoost minimises the exponential loss (eq. $\\eqref{eq:exp_loss}$) via a forward-stagewise additive modelling approach.**\n",
    "\n",
    "**AdaBoost is not optimising training set misclassification error.** So it is common that the misclassification error of training set has decreased to 0 at XXX rounds of iterations, and the exponential loss keeps decreasing after XXX rounds (and the error on testing data keeps decreasing as well). \n",
    "The exponential loss is more sensitive to changes in the estimated class probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only unsolved is the update rule for the  \"coefficient\" $w_i^{(m)}$.\n",
    "\\begin{align}\n",
    "w_i^{(m+1)}&=e^{-y_i f_{m}(x_i)} \\\\\n",
    "&=e^{-y_i [f_{m-1}(x_i)+\\beta_m G_m(x_i)]} \\\\\n",
    "&= e^{-\\beta_m y_iG_m(x_i)} e^{-y_i f_{m-1}(x_i)} \\\\\n",
    "&= e^{-\\beta_m y_iG_m(x_i)} w_i^{(m)} \\\\\n",
    "&= e^{-\\beta_m(1-2I(y_i \\neq G_m(x_i))} w_i^{(m)} \\\\\n",
    "&= e^{2\\beta_m I(y_i \\neq G_m(x_i))} e^{-\\beta_m} w_i^{(m)}\n",
    "\\label{eq:9} \\tag{9}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "y_iG_m(x_i)&=1, y_i = G_m(x_i) \\\\\n",
    "y_iG_m(x_i)&=-1, y_i \\neq G_m(x_i) \\\\\n",
    "y_iG_m(x_i)&= 1-2I(y_i \\neq G_m(x_i))\n",
    "\\end{align}\n",
    "\n",
    "**Eq. $\\eqref{eq:9}$** is the equation to get the examples' weights at next iteration $w_i^{(m+1)}$ with $w_i^{(m)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a AdaBoost algorithm step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the AdaBoost\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def myAdaBoost(X_train, y_train, X_test, y_test, weak_learner, m_iteration=200):\n",
    "    \"\"\"\n",
    "    Implementation of AdaBoost.M1, Freund and Schapire (1997), see TESL pp.339\n",
    "    two-class prediction problem, with the classes are -1 and 1 \n",
    "    and the final prediction uses sign(wl_weight*wl_prediction)\n",
    "    \"\"\"\n",
    "    # 0. preprocess: y label to -1 and 1\n",
    "    ori_labels = np.unique(y_train).astype(int)\n",
    "    if len(ori_labels) != 2:\n",
    "        print('Cannot handle >2 classes')\n",
    "        exit()\n",
    "    elif np.all(ori_labels==np.array([-1,1])):\n",
    "        pass\n",
    "    else:\n",
    "        print('reset lables to -1 or 1')\n",
    "        y_train = np.where(y_train==ori_labels[0],-1,1)\n",
    "        y_test = np.where(y_test==ori_labels[0],-1,1)\n",
    "    \n",
    "    # 1. initialise weights for each of the N examples, w_i=1/N\n",
    "    N, N_test = len(X_train), len(X_test)\n",
    "    w = np.ones(X_train.shape[0])/N\n",
    "    \n",
    "    # 2. boosting\n",
    "    \n",
    "    predictions, test_predictions = [], []\n",
    "    wl_weights = []  # weights of each weak learners\n",
    "    \n",
    "    for m in range(1, m_iteration+1):\n",
    "        # (a). train a weak classfier G_m(x) to training data with weights w\n",
    "        weak_learner.fit(X_train, y_train, sample_weight=w)\n",
    "        \n",
    "        # wls.append(weak_learner)  # !!! model append in loop does not work?!!!\n",
    "        \n",
    "        # (b). calculate error for G_m(x)\n",
    "        y_pre = weak_learner.predict(X_train)\n",
    "        predictions.append(y_pre)\n",
    "        error = np.sum(w*1*(y_pre != y_train)) / np.sum(w)\n",
    "        \n",
    "        # for testing\n",
    "        y_test_pre = weak_learner.predict(X_test)\n",
    "        test_predictions.append(y_test_pre)\n",
    "        \n",
    "        # (c). compute alpha_m\n",
    "        alpha_m = np.log((1-error)/error)\n",
    "        wl_weights.append(alpha_m)\n",
    "        \n",
    "        # (d). update training examples' weights\n",
    "        w = w * np.exp(alpha_m*1*(y_pre != y_train))\n",
    "        \n",
    "    # 3. get the weighted majority vote\n",
    "    wl_weights = np.array(wl_weights)  # m\n",
    "    print('model weights (first three): ', wl_weights[:3])\n",
    "    \n",
    "    \n",
    "    # training\n",
    "    predictions = np.array(predictions) # m*N, predict value is either 1 or -1\n",
    "    train_final = np.sign(np.sum(np.transpose(predictions)*wl_weights, axis=1))\n",
    "    print('Train accuracy', np.sum(y_train==train_final)/N)\n",
    "    \n",
    "    \n",
    "    # testing\n",
    "    test_predictions = np.array(test_predictions)\n",
    "    test_final = np.sign(np.sum(np.transpose(test_predictions)*wl_weights, axis=1))\n",
    "    print('Test accuracy', np.sum(y_test==test_final)/N_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testcase from 李航 《统计机器学习》"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset lables to -1 or 1\n",
      "model weights (first three):  [0.84729786 1.29928298 1.5040774 ]\n",
      "Train accuracy 1.0\n",
      "Test accuracy 0.75\n"
     ]
    }
   ],
   "source": [
    "# 1. let's define a weak learner: 1-depth decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_tree = DecisionTreeClassifier(max_depth = 1, random_state = 1)\n",
    "\n",
    "# 2. set up train and test examples\n",
    "X_train = np.arange(10).reshape(10,1)\n",
    "y_train = np.array([1,1,1,-1,-1,-1,1,1,1,-1])-6\n",
    "\n",
    "X_test = np.array([5.8, 2.1, 4.6, 7.9]).reshape(4,1)\n",
    "y_test = np.array([-1, 1, -1, 1])\n",
    "\n",
    "# 3. use AdaBoost\n",
    "myAdaBoost(X_train, y_train, X_test, y_test, clf_tree, m_iteration=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testcase from [sklearn.datasets.make_hastie_10_2](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_hastie_10_2.html#sklearn.datasets.make_hastie_10_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(np.unique(y_train).astype(int)==np.array([-1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model weights (first three):  [0.17645644 0.16017128 0.24968398]\n",
      "Train accuracy 0.9415\n",
      "Test accuracy 0.884\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "X, y = datasets.make_hastie_10_2(n_samples=12000, random_state=1)\n",
    "\n",
    "X_test, y_test = X[2000:], y[2000:]\n",
    "X_train, y_train = X[:2000], y[:2000]\n",
    "\n",
    "myAdaBoost(X_train, y_train, X_test, y_test, clf_tree, m_iteration=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Two-class AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_gaussian_quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, y1 = make_gaussian_quantiles(cov=2., n_samples=200, n_features=2,\n",
    "                                n_classes=2, random_state=1)\n",
    "X2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5,\n",
    "                                 n_samples=300, n_features=2,\n",
    "                                 n_classes=2, random_state=1)\n",
    "X = np.concatenate((X1, X2))\n",
    "y = np.concatenate((y1, -y2+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(list(y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AdaBoosted DT\n",
    "bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), algorithm=\"SAMME\",n_estimators=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit\n",
    "bdt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean accuracy of self.predict(X) wrt. y.\n",
    "bdt.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = len(bdt)\n",
    "\n",
    "# error at each iteration\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(range(1, n_trees + 1), bdt.estimator_errors_[:n_trees],\n",
    "         \"b\", label='SAMME', alpha=.5)\n",
    "plt.legend()\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Number of Trees')\n",
    "\n",
    "# boost weight of each tree\n",
    "plt.subplot(122)\n",
    "plt.plot(range(1, n_trees + 1), bdt.estimator_weights_[:n_trees],\n",
    "         \"b\", label='SAMME', alpha=.5)\n",
    "plt.legend()\n",
    "plt.ylabel('Weight')\n",
    "plt.xlabel('Number of Trees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_colors = \"br\"\n",
    "plot_step = 0.02\n",
    "class_names = \"AB\"\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "# Plot the decision boundaries\n",
    "# plt.subplot(121)\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                     np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "Z = bdt.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "plt.axis(\"tight\")\n",
    "\n",
    "# Plot the training points\n",
    "for i, n, c in zip(range(2), class_names, plot_colors):\n",
    "    idx = np.where(y == i)\n",
    "    plt.scatter(X[idx, 0], X[idx, 1],\n",
    "                c=c, cmap=plt.cm.Paired,\n",
    "                s=20, edgecolor='k',\n",
    "                label=\"Class %s\" % n)\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Decision Boundary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the two-class decision scores\n",
    "twoclass_output = bdt.decision_function(X)\n",
    "plot_range = (twoclass_output.min(), twoclass_output.max())\n",
    "plt.figure(figsize=(5,5))\n",
    "for i, n, c in zip(range(2), class_names, plot_colors):\n",
    "    plt.hist(twoclass_output[y == i],\n",
    "             bins=10,\n",
    "             range=plot_range,\n",
    "             facecolor=c,\n",
    "             label='Class %s' % n,\n",
    "             alpha=.5,\n",
    "             edgecolor='k')\n",
    "x1, x2, y1, y2 = plt.axis()\n",
    "plt.axis((x1, x2, y1, y2 * 1.2))\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Samples')\n",
    "plt.xlabel('Score')\n",
    "plt.title('Decision Scores')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-4-prac]",
   "language": "python",
   "name": "conda-env-py3-4-prac-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
